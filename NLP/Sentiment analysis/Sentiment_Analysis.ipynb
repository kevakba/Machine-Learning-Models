{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding  "
      ],
      "metadata": {
        "id": "SCXU-nVTzT4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "reviews = []\n",
        "for line in open('/content/Musical_Instruments_5.json', 'r'):\n",
        "    reviews.append(json.loads(line))"
      ],
      "metadata": {
        "id": "YXtqn7SYgquj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the entry in the datset\n",
        "reviews[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AqinlcZgqrP",
        "outputId": "cd522652-7a74-48eb-dd15-bbf953835a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'asin': '1384719342',\n",
              "  'helpful': [0, 0],\n",
              "  'overall': 5.0,\n",
              "  'reviewText': \"Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\",\n",
              "  'reviewTime': '02 28, 2014',\n",
              "  'reviewerID': 'A2IBPI20UZIR0U',\n",
              "  'reviewerName': 'cassandra tu \"Yeah, well, that\\'s just like, u...',\n",
              "  'summary': 'good',\n",
              "  'unixReviewTime': 1393545600}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the review text columns and sanity checking\n",
        "review_text = [review['reviewText'] for review in reviews]\n",
        "review_text[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqdqMwMdgqo1",
        "outputId": "92998140-d4d3-4f27-c88d-529fcfc3f933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\",\n",
              " \"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the rating column and sanity check\n",
        "ratings = [float(review['overall']) for review in reviews]\n",
        "ratings[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkt5pBOHgqme",
        "outputId": "c20d1628-ada4-4aa4-e91e-a17cd23bbc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.0, 5.0]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qodeiHgQxkcl",
        "outputId": "252bc711-b3c9-4fab-f1b9-af13f8a1e122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1.0, 2.0, 3.0, 4.0, 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above, we note that there are 5 unique values of ratings column. Let us frame a rule wherein if the rating is more than 2.0 then the sentiment is deemed as positive while it is negative otherwise."
      ],
      "metadata": {
        "id": "RqqlEqbOxn2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating sentiment data\n",
        "sentiment = [1 if r>2 else 0 for r in ratings]"
      ],
      "metadata": {
        "id": "YeQoeB6Whjas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting all the words of total data\n",
        "word_dict = [word for sent in review_text for word in sent.split(' ')]\n",
        "\n",
        "# checking the total dictionary size of the reviews corpus\n",
        "dict_size = len(set(word_dict))\n",
        "dict_size"
      ],
      "metadata": {
        "id": "7RPb_fEMhy65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b629175a-e662-4806-cb52-4597940b6097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57715"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will encode each words in the sentence with the tf.keras onehot encooding. One-hot encodes a text into a list of word indexes of size dictionary."
      ],
      "metadata": {
        "id": "Tw4yz3PbzZvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodsent = [one_hot(sent, dict_size) for sent in review_text]"
      ],
      "metadata": {
        "id": "KVw3G11Ghy1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the matrix length of review\n",
        "max_len_sentence = max([len(sent) for sent in encodsent])\n",
        "max_len_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMLvciWvhyzE",
        "outputId": "606ea0de-b8df-4987-9801-eadac2c5a62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2059"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will set the size of review as max and padd the other review with zeros, so that there reviews could be fed into our model."
      ],
      "metadata": {
        "id": "FUVCRbRLzlHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padmysent = pad_sequences(encodsent, maxlen=max_len_sentence, padding='pre')\n",
        "print(padmysent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNqennjNhywe",
        "outputId": "159329e0-ed9b-4772-d5ed-25b513e641e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0     0     0 ... 12803  6611 52210]\n",
            " [    0     0     0 ... 41378  3644 38023]\n",
            " [    0     0     0 ... 34239 52947 39297]\n",
            " ...\n",
            " [    0     0     0 ... 52947 30942 32089]\n",
            " [    0     0     0 ... 51034 53601 15784]\n",
            " [    0     0     0 ... 18973 18973 56253]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "\n",
        "train_size = int(len(padmysent)*.8)\n",
        "X_train, X_test = padmysent[:train_size], padmysent[train_size:]\n",
        "y_train, y_test = np.array(sentiment[:train_size]), np.array(sentiment[train_size:])"
      ],
      "metadata": {
        "id": "wcJJIB76jFEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model building\n",
        "mymodel = Sequential()\n",
        "\n",
        "# this layer turns positive integers (indexes) into dense vectors of size fed. Here the size is 10\n",
        "mymodel.add(Embedding(dict_size, 10, input_length=max_len_sentence))\n",
        "\n",
        "mymodel.add(Flatten())\n",
        "\n",
        "mymodel.add(Dense(1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "GsEFehQ4hyuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling the model\n",
        "mymodel.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "QQp-a6CZhyrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have set loss as `binary_crossentropy` (Computes the cross-entropy loss between true labels and predicted labels.) and metric value as `accuracy`"
      ],
      "metadata": {
        "id": "eF7svVjy0zOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the model summary\n",
        "mymodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t8pJRoThyos",
        "outputId": "3980056b-b9a9-41bd-8511-2ec04857d6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 2059, 10)          577150    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 20590)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 20591     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 597,741\n",
            "Trainable params: 597,741\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "mymodel.fit(X_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKPxxVMbhymD",
        "outputId": "3b32a70f-ff1d-4984-8514-a5ae0e122b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "257/257 [==============================] - 5s 14ms/step - loss: 0.1869 - accuracy: 0.9564\n",
            "Epoch 2/5\n",
            "257/257 [==============================] - 4s 14ms/step - loss: 0.1582 - accuracy: 0.9564\n",
            "Epoch 3/5\n",
            "257/257 [==============================] - 3s 13ms/step - loss: 0.1039 - accuracy: 0.9626\n",
            "Epoch 4/5\n",
            "257/257 [==============================] - 3s 13ms/step - loss: 0.0615 - accuracy: 0.9781\n",
            "Epoch 5/5\n",
            "257/257 [==============================] - 4s 14ms/step - loss: 0.0388 - accuracy: 0.9883\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe126dbc090>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting\n",
        "pred = mymodel.predict(X_test)"
      ],
      "metadata": {
        "id": "X3K2um3vhjYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHnou5N21JTk",
        "outputId": "03ffdaa1-8d18-4cbf-fab6-94fa7b303223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9788058],\n",
              "       [0.9960004],\n",
              "       [0.9993549],\n",
              "       [0.9978851],\n",
              "       [0.9938103]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the output is probabilties. So we need to convert them back the firm labels to check the metric values. Lets set a rule wherein class is 1 if probability if more than 0.5 and 0 otherwise. We can of course change the same to achieve higher metric value."
      ],
      "metadata": {
        "id": "skY3XuL21IBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thres=0.5\n",
        "pred_label = [1 if p[0]>thres else 0 for p in pred]"
      ],
      "metadata": {
        "id": "O23tF9-7hjVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the different metric value\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test, pred_label))\n",
        "print(accuracy_score(y_test, pred_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLHu2PhVkAk9",
        "outputId": "88dc3429-8a49-411d-aa34-2007ef8543e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   8  101]\n",
            " [   0 1944]]\n",
            "0.950803701899659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:** Form the above, we conclude that we can train a basic sentiment analysis model with few number of iterations and a reliable text corpus."
      ],
      "metadata": {
        "id": "iPDARDgt1rNb"
      }
    }
  ]
}